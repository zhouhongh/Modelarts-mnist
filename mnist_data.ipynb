{"cells": [{"metadata": {"trusted": false}, "cell_type": "code", "source": "from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport moxing.tensorflow as mox\nimport gzip\nimport os\n\nimport numpy\nfrom scipy import ndimage\n\nfrom six.moves import urllib\n\nimport tensorflow as tf\ntf.reset_default_graph()", "execution_count": 39, "outputs": []}, {"metadata": {"trusted": false}, "cell_type": "code", "source": "####### your coding place\uff1a begin  ###########\n# \u6b64\u5904\u5fc5\u987b\u4fee\u6539\u4e3a\u7528\u6237\u6570\u636e\u6876\u4f4d\u7f6e\n\n#\u6570\u636e\u5728OBS\u7684\u5b58\u50a8\u4f4d\u7f6e\u3002\n# eg. s3:// \uff1a\u7edf\u4e00\u8def\u5f84\u8f93\u5165\n#     /uBucket \uff1a\u6876\u540d\uff0c\u7528\u6237\u7684\u79c1\u6709\u6876\u7684\u540d\u79f0 eg. bucket\n#     /notebook/data/\uff1a \u6587\u4ef6\u8def\u5f84\n\ndata_url = 's3://zhh-obs001/test-modelarts/dataset-mnist/' \n\n####### your coding place\uff1a end  ###########", "execution_count": 40, "outputs": []}, {"metadata": {"trusted": false}, "cell_type": "code", "source": "# \u672c\u5730\u521b\u5efa\u6570\u636e\u5b58\u50a8\u6587\u4ef6\u5939\nlocal_url = './cache/local_data/'\nif mox.file.exists(local_url):\n    mox.file.remove(local_url,recursive=True)\nos.makedirs(local_url)\n\n#\u5c06\u79c1\u6709\u6876\u4e2d\u7684\u6570\u636e\u62f7\u8d1d\u5230\u672c\u5730mox.file.copy_parallel\uff08\uff09\n\"\"\"\n  Copy all files in src_url to dst_url. Same usage as `shutil.copytree`.\n  Note that this method can only copy a directory. If you want to copy a single file,\n  please use `mox.file.copy`\n\n  Example::\n\n    copy_parallel(src_url='/tmp', dst_url='s3://bucket_name/my_data')\n\n  Assuming files in `/tmp` are:\n\n  * /tmp:\n      * |- train\n          * |- 1.jpg\n          * |- 2.jpg\n      * |- eval\n          * |- 3.jpg\n          * |- 4.jpg\n\n  Then files after copy in `s3://bucket_name/my_data` are:\n\n  * s3://bucket_name/my_data:\n      * |- train\n          * |- 1.jpg\n          * |- 2.jpg\n      * |- eval\n          * |- 3.jpg\n          * |- 4.jpg\n\n  Directory `tmp` will not be copied. If `file_list` is `['train/1.jpg', 'eval/4.jpg']`,\n  then files after copy in `s3://bucket_name/my_data` are:\n\n  * s3://bucket_name/my_data\n      * |- train\n          * |- 1.jpg\n      * |- eval\n          * |- 4.jpg\n\n  :param src_url: Source path or s3 url\n  :param dst_url: Destination path or s3 url\n  :param file_list: A list of relative path to `src_url` of files need to be copied.\n  :param threads: Number of threads or processings in Pool.\n  :param is_processing: If True, multiprocessing is used. If False, multithreading is used.\n  :param use_queue: Whether use queue to manage downloading list.\n  :return: None\n\"\"\"\nmox.file.copy_parallel(data_url, local_url)\ndata_url = local_url\nos.listdir(data_url)", "execution_count": 42, "outputs": [{"output_type": "execute_result", "execution_count": 42, "data": {"text/plain": "['t10k-images-idx3-ubyte',\n 't10k-images-idx3-ubyte.gz',\n 't10k-labels-idx1-ubyte',\n 't10k-labels-idx1-ubyte.gz',\n 'train-labels-idx1-ubyte.gz',\n 'train-images-idx3-ubyte',\n 'train-images-idx3-ubyte.gz',\n 'train-labels-idx1-ubyte']"}, "metadata": {}}]}, {"metadata": {"trusted": false}, "cell_type": "code", "source": "DATA_DIRECTORY = data_url\n# Params for MNIST\nIMAGE_SIZE = 28\nNUM_CHANNELS = 1\nPIXEL_DEPTH = 255\nNUM_LABELS = 10\nVALIDATION_SIZE = 5000  # Size of the validation set.\n\n", "execution_count": 45, "outputs": []}, {"metadata": {"trusted": false}, "cell_type": "code", "source": "def maybe_download(filename):\n    \"\"\"Download the data from Yann's website, unless it's already here.\"\"\"\n    if not tf.gfile.Exists(DATA_DIRECTORY):\n        tf.gfile.MakeDirs(DATA_DIRECTORY)\n    filepath = os.path.join(DATA_DIRECTORY, filename)\n    if not tf.gfile.Exists(filepath):\n        print(filepath,\" does not exist\")\n        filepath, _ = urllib.request.urlretrieve(SOURCE_URL + filename, filepath)\n        with tf.gfile.GFile(filepath) as f:\n            size = f.size()\n        print('Successfully downloaded', filename, size, 'bytes.')\n    return filepath\n\n# Extract the images\ndef extract_data(filename, num_images):\n    \"\"\"Extract the images into a 4D tensor [image index, y, x, channels].\n\n    Values are rescaled from [0, 255] down to [-0.5, 0.5].\n    \"\"\"\n    print('Extracting', filename)\n    with gzip.open(filename) as bytestream:\n        bytestream.read(16)\n        buf = bytestream.read(IMAGE_SIZE * IMAGE_SIZE * num_images * NUM_CHANNELS)\n        data = numpy.frombuffer(buf, dtype=numpy.uint8).astype(numpy.float32)\n        data = (data - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH\n        data = data.reshape(num_images, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)\n        data = numpy.reshape(data, [num_images, -1])\n    return data\n\n# Extract the labels\ndef extract_labels(filename, num_images):\n    \"\"\"Extract the labels into a vector of int64 label IDs.\"\"\"\n    print('Extracting', filename)\n    with gzip.open(filename) as bytestream:\n        bytestream.read(8)\n        buf = bytestream.read(1 * num_images)\n        labels = numpy.frombuffer(buf, dtype=numpy.uint8).astype(numpy.int64)\n        num_labels_data = len(labels)\n        one_hot_encoding = numpy.zeros((num_labels_data,NUM_LABELS))\n        one_hot_encoding[numpy.arange(num_labels_data),labels] = 1\n        one_hot_encoding = numpy.reshape(one_hot_encoding, [-1, NUM_LABELS])\n    return one_hot_encoding\n\n# Augment training data\ndef expend_training_data(images, labels):\n\n    expanded_images = []\n    expanded_labels = []\n\n    j = 0 # counter\n    for x, y in zip(images, labels):\n        j = j+1\n        if j%100==0:\n            print ('expanding data : %03d / %03d' % (j,numpy.size(images,0)))\n\n        # register original data\n        expanded_images.append(x)\n        expanded_labels.append(y)\n\n        # get a value for the background\n        # zero is the expected value, but median() is used to estimate background's value \n        bg_value = numpy.median(x) # this is regarded as background's value        \n        image = numpy.reshape(x, (-1, 28))\n\n        for i in range(4):\n            # rotate the image with random degree\n            angle = numpy.random.randint(-15,15,1)\n            new_img = ndimage.rotate(image,angle,reshape=False, cval=bg_value)\n\n            # shift the image with random distance\n            shift = numpy.random.randint(-2, 2, 2)\n            new_img_ = ndimage.shift(new_img,shift, cval=bg_value)\n\n            # register new training data\n            expanded_images.append(numpy.reshape(new_img_, 784))\n            expanded_labels.append(y)\n\n    # images and labels are concatenated for random-shuffle at each epoch\n    # notice that pair of image and label should not be broken\n    expanded_train_total_data = numpy.concatenate((expanded_images, expanded_labels), axis=1)\n    numpy.random.shuffle(expanded_train_total_data)\n\n    return expanded_train_total_data\n\n# Prepare MNISt data\ndef prepare_MNIST_data(use_data_augmentation=True):\n    # Get the data.\n    train_data_filename = maybe_download('train-images-idx3-ubyte.gz')\n    train_labels_filename = maybe_download('train-labels-idx1-ubyte.gz')\n    test_data_filename = maybe_download('t10k-images-idx3-ubyte.gz')\n    test_labels_filename = maybe_download('t10k-labels-idx1-ubyte.gz')\n\n    # Extract it into numpy arrays.\n    train_data = extract_data(train_data_filename, 60000)\n    train_labels = extract_labels(train_labels_filename, 60000)\n    test_data = extract_data(test_data_filename, 10000)\n    test_labels = extract_labels(test_labels_filename, 10000)\n\n    # Generate a validation set.\n    validation_data = train_data[:VALIDATION_SIZE, :]\n    validation_labels = train_labels[:VALIDATION_SIZE,:]\n    train_data = train_data[VALIDATION_SIZE:, :]\n    train_labels = train_labels[VALIDATION_SIZE:,:]\n\n    # Concatenate train_data & train_labels for random shuffle\n    if use_data_augmentation:\n        train_total_data = expend_training_data(train_data, train_labels)\n    else:\n        train_total_data = numpy.concatenate((train_data, train_labels), axis=1)\n\n    train_size = train_total_data.shape[0]\n\n    return train_total_data, train_size, validation_data, validation_labels, test_data, test_labels\n", "execution_count": 46, "outputs": []}], "metadata": {"kernelspec": {"name": "tensorflow-1.8", "display_name": "TensorFlow-1.8", "language": "python"}, "language_info": {"name": "python", "version": "3.6.4", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}